# -*- coding: utf-8 -*-
"""Final Project AlgoWizard

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Tj7rG9X6YnKs9Q3Yxgzkq-ZjcMfO3KR

# 1. Import Library
"""

!pip install streamlit
import pandas as pd  # Data manipulation and analysis
import numpy as np  # Numerical operations on data
import matplotlib.pyplot as plt  # Data visualization
import seaborn as sns  # Data visualization

from sklearn.preprocessing import StandardScaler  # Perform scaling
from sklearn.model_selection import train_test_split  # Split data into training and testing sets

# ML model
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier, LocalOutlierFactor
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # Measure model accuracy and display summary statistics of model performance
import streamlit as st  # Model deployment

from imblearn.over_sampling import SMOTE

import itertools

"""# 2. Analisis Data Eksploratif (EDA) & Pra-Pemrosesan Data

PIC : July, Ola, Osha, Faza

## 2.1 Membaca Dataset
"""

# Mengambil dataset dari URL
url = "https://raw.githubusercontent.com/nuraulaola/ds-batch-32-final-project/main/datasets/Dataset1_Customer_Churn.csv"
df = pd.read_csv(url)

# Menampilkan lima baris pertama dari dataset untuk memahami struktur data
df.head()

"""Deskripsi Data

Cerita Dataset:

- Dataset ini terdiri dari total 10.000 baris (entries) dengan rentang indeks baris dari 0 hingga 9999, dan terdapat 7 kolom.
- Variabel independen melibatkan Gender, Age, CreditScore, EstimatedSalary, dan HasCrCard, yang berisi informasi tentang pelanggan.
- Variabel dependen adalah Exited, yang menunjukkan apakah pelanggan tersebut telah meninggalkan layanan.

Fitur:

- CustomerId: ID unik untuk setiap pelanggan.
- Gender: Jenis kelamin pelanggan (Female / Male).
- Age: Usia pelanggan.
- CreditScore: Skor kredit pelanggan.
- EstimatedSalary: Perkiraan gaji pelanggan.
- HasCrCard: Menunjukkan apakah pelanggan memiliki kartu kredit (1 untuk ya, 0 untuk tidak).
- Exited: Variabel target yang menunjukkan apakah pelanggan telah keluar dari layanan (1 untuk ya, 0 untuk tidak).
"""

# Mendefinisikan nama variabel dependen yang merupakan target analisis
dependent_variable_name = "Exited"

"""## 2.2 EDA

### 2.2.1 Analisis Missing Values dan Jumlah Nilai Unik di Setiap Kolom
"""

# Memeriksa missing values
missing_values = df.isnull().sum()

# Menampilkan jumlah missing values
print("Jumlah missing values:")
print(missing_values)

# Menampilkan jumlah nilai unik dari setiap variabel dalam dataframe
def show_unique_count_variables(df):
    # Menghitung jumlah nilai unik
    unique_counts = df.nunique()

    # Menampilkan jumlah nilai unik
    print("\nJumlah nilai unik:")
    print(unique_counts)

# Menggunakan fungsi untuk menampilkan jumlah nilai unik
show_unique_count_variables(df)

"""### 2.2.2 Analisis Nilai-Nilai Yang Duplikat"""

# Memeriksa nilai-nilai yang duplikat
duplicate_rows = df[df.duplicated()]

# Menampilkan baris yang merupakan duplikat
print("Duplikat dalam DataFrame:")
print(duplicate_rows)

"""### 2.2.3 Overview Tipe Data dari Setiap Kolom"""

# Memeriksa tipe data dari setiap kolom
column_data_types = df.dtypes

# Menampilkan tipe data dari setiap kolom
print("Tipe data dari setiap kolom:")
print(column_data_types)

"""### 2.2.4 Informasi Umum DataFrame"""

# Menampilkan informasi umum mengenai DataFrame
print("Informasi umum mengenai DataFrame:")
df.info()

"""### 2.2.5 Rangkuman Statistik Deskriptif"""

# Menampilkan statistik deskriptif singkat
print("Statistik deskriptif dari kolom numerik DataFrame:")
df.describe()

"""### 2.2.6 Plot Distribusi Variabel Dependen Terhadap Beberapa Variabel Independen"""

# Menampilkan distribusi variabel dependen terhadap beberapa variabel independen
fig, axarr = plt.subplots(2, 3, figsize=(18, 6))

# Menampilkan distribusi variabel dependen 'Exited' terhadap variabel independen 'Gender'
sns.histplot(x='Gender', hue='Exited', data=df, ax=axarr[0, 0], multiple='stack', kde=True)

# Menampilkan distribusi variabel dependen 'Exited' terhadap variabel independen 'Age'
sns.histplot(x='Age', hue='Exited', data=df, ax=axarr[0, 1], multiple='stack', kde=True)

# Menampilkan distribusi variabel dependen 'Exited' terhadap variabel independen 'CreditScore'
sns.histplot(x='CreditScore', hue='Exited', data=df, ax=axarr[0, 2], multiple='stack', kde=True)

# Menampilkan distribusi variabel dependen 'Exited' terhadap variabel independen 'EstimatedSalary'
sns.histplot(x='EstimatedSalary', hue='Exited', data=df, ax=axarr[1, 0], multiple='stack', kde=True)

# Menampilkan distribusi variabel dependen 'Exited' terhadap variabel independen 'HasCrCard'
sns.histplot(x='HasCrCard', hue='Exited', data=df, ax=axarr[1, 1], multiple='stack', kde=True)

# Menghitung jumlah nilai 0 (tidak Exited) dan nilai 1 (Exited) pada variabel dependen 'Exited'
zero_count, one_count = df['Exited'].value_counts()
print("Distribusi variabel dependen terhadap beberapa variabel independen:")
print("Exited 0 count:", zero_count)
print("Exited 1 count:", one_count)

fig.suptitle('Distribusi Variabel Dependen Terhadap Beberapa Variabel Independen', fontsize=18)

# Menampilkan plot
plt.show()

"""1. **Distribusi variabel dependen 'Exited' terhadap variabel independen 'Gender':**
- Female memiliki jumlah yang lebih rendah pada kategori '1' (Exited), yang sesuai dengan rendahnya nilai KDE pada kategori tertentu.
- Male memiliki jumlah yang lebih tinggi pada kategori '0' (Not Exited), yang sesuai dengan tingginya nilai KDE pada kategori tertentu.

2. **Distribusi variabel dependen 'Exited' terhadap variabel independen 'Age':**
- Terdapat peningkatan jumlah Exited dari rentang usia '0-18' hingga '50+', dengan puncaknya pada kategori '50+'. (terlihat dari bentuk distribusi normal yang miring ke kanan / right-skewed)
- Distribusi yang miring ke kanan mencerminkan bahwa sebagian besar nasabah berada di kelompok usia yang lebih muda. Fakta bahwa rentang usia '19-35' memiliki jumlah Exited yang tinggi menunjukkan bahwa di antara nasabah muda, tingkat churn cenderung lebih tinggi.
- Distribusi yang miring ke kanan dapat menunjukkan bahwa meskipun kebanyakan nasabah berada di kelompok usia yang lebih muda, ada juga sejumlah nasabah yang lebih tua di rentang '36-50'. Jumlah Exited yang tinggi di rentang ini menunjukkan bahwa bahkan di antara nasabah yang lebih tua, tingkat churn tetap signifikan.

3. **Distribusi variabel dependen 'Exited' terhadap variabel independen 'CreditScore':**
- Bentuk distribusi normal menunjukkan bahwa sebagian besar nasabah memiliki CreditScore yang berpusat di sekitar mean.
- Namun, lonjakan di atas sumbu x pada nilai 800 mengindikasikan adanya kelompok kecil tetapi signifikan dari nasabah dengan CreditScore sangat tinggi.

4. **Distribusi variabel dependen 'Exited' terhadap variabel independen 'EstimatedSalary':**
- Distribusi seragam EstimatedSalary menunjukkan bahwa estimasi pendapatan nasabah cenderung stabil dan tidak mengalami variasi yang signifikan.
- Meskipun demikian, tidak ada pola khusus atau tren yang terlihat dalam hubungannya dengan tingkat churn. Hal ini mengindikasikan bahwa faktor-faktor lain di luar estimasi pendapatan mungkin lebih berperan dalam keputusan nasabah untuk bertahan atau keluar.

5. **Distribusi variabel dependen 'Exited' terhadap variabel independen 'HasCrCard':**
- Terdapat perbedaan yang signifikan antara pemegang kartu kredit (HasCrCard=1) dan bukan pemegang kartu kredit (HasCrCard=0) dalam hal jumlah Exited.
- Pemegang kartu kredit (HasCrCard=1) memiliki jumlah Exited yang lebih tinggi dibandingkan dengan yang bukan pemegang kartu kredit.

### 2.2.7 Plot Distribusi Variabel Numerik
"""

# Menampilkan distribusi variabel numerik dalam DataFrame
numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
df_num_cols = df.select_dtypes(include=numerics)
columns = df_num_cols.columns[: len(df_num_cols.columns)]

fig = plt.figure()
fig.set_size_inches(18, 15)
length = len(columns)

for i, j in zip(columns, range(length)):
    plt.subplot(int(length / 2), 3, j + 1)
    plt.subplots_adjust(wspace=0.2, hspace=0.5)
    sns.histplot(df_num_cols[i], bins=20, kde=True, color='skyblue', edgecolor='black')
    plt.title(i)

fig.suptitle('Distribusi Variabel Numerik', fontsize=18)
plt.show()

"""1. **Distribusi Variabel "CustomerId":**
- Distribusi variabel "CustomerId" cenderung seragam, dengan jumlah frekuensi relatif stabil di setiap bin edge.

2. **Distribusi Variabel "Age":**
- Distribusi variabel "Age" menunjukkan adanya puncak di sebelah kiri dan landai ke kanan (skewed right).
- Rentang usia dari '19-35' memiliki jumlah frekuensi "Exited" yang lebih tinggi dibandingkan dengan '0-18', tetapi menurun untuk '36-50'.

3. **Distribusi Variabel "CreditScore":**
- Distribusi variabel "CreditScore" memiliki bentuk yang mirip dengan distribusi normal, tetapi terdapat peningkatan yang tiba-tiba melonjak di atas nilai 800.

4. **Distribusi Variabel "EstimatedSalary":**
- Distribusi variabel "EstimatedSalary" cenderung seragam, dengan jumlah frekuensi yang relatif stabil di setiap bin edge.

5. **Distribusi Variabel "HasCrCard":**
- Distribusi variabel "HasCrCard" menunjukkan bahwa pemegang kartu kredit (HasCrCard=1) memiliki jumlah frekuensi "Exited" yang lebih tinggi dibandingkan dengan yang bukan pemegang kartu kredit (HasCrCard=0).

6. **Distribusi Variabel "Exited":**
- Distribusi variabel "Exited" menunjukkan bahwa mayoritas data memiliki label 0 (Not Exited) dengan frekuensi yang tinggi, sementara jumlah frekuensi label 1 (Exited) hanya muncul di akhir (nilai 1.00).

### 2.2.8 Plot Distribusi Variabel Numerik (Spesifik Pelanggan yang Keluar Layanan)
"""

# Menampilkan distribusi variabel dependen terhadap variabel lainnya
numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
df_dependent_var = df[df['Exited']==1]
df_num_cols = df_dependent_var.select_dtypes(include=numerics)
columns = df_num_cols.columns[:len(df_num_cols.columns)]
fig = plt.figure()
fig.set_size_inches(18, 15)
length = len(columns)
for i, j in zip(columns, range(length)):
    plt.subplot(int(length/2), 3, j+1)
    plt.subplots_adjust(wspace=0.2, hspace=0.5)
    sns.histplot(df_num_cols[i], bins=20, kde=True, color='skyblue', edgecolor='black')
    plt.title(i)
fig.suptitle('Distribusi Variabel Dependen Terhadap Variabel Lainnya', fontsize=18)
plt.show()

"""1. **Variabel "CustomerId":**
- Distribusi variabel "CustomerId" menunjukkan bentuk plot yang seragam, dengan frekuensi yang relatif stabil di setiap bin edge.

2. **Variabel "Age":**
- Distribusi variabel "Age" menunjukkan bentuk plot yang normal dengan sedikit right-skewed.
- Mayoritas data berada dalam rentang usia '36-50'.

3. **Variabel "CreditScore":**
- Distribusi variabel "CreditScore" memiliki bentuk plot yang normal, namun sedikit left-skewed.

4. **Variabel "EstimatedSalary":**
- Distribusi variabel "EstimatedSalary" menunjukkan bentuk plot yang seragam, dengan frekuensi yang relatif stabil di setiap bin edge.

5. **Variabel "HasCrCard":**
- Distribusi variabel "HasCrCard" menunjukkan bahwa mayoritas data berada pada kategori pemegang kartu kredit (HasCrCard=1).
- Pemegang kartu kredit (HasCrCard=1) memiliki jumlah frekuensi "Exited" yang lebih tinggi dibandingkan dengan yang bukan pemegang kartu kredit (HasCrCard=0).

6. **Variabel "Exited":**
- Dalam plot distribusi variabel "Exited", tidak ditampilkan frekuensi yang berkaitan dengan pelanggan yang tidak keluar dari layanan ("Exited"=0), dan fokusnya adalah pada pelanggan yang telah keluar ("Exited"=1).

### 2.2.9 Korelasi antar Variabel
"""

# Matriks korelasi
correlation_matrix = df.corr()

# Heatmap korelasi
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="Blues", fmt=".2f", linewidths=0.5)
plt.title('Matriks Korelasi Antar Variabel')
plt.show()

"""1. **Korelasi antara Umur (Age) dan Exited:**
- Korelasi positif sebesar 0.36 menunjukkan bahwa ada hubungan yang moderat antara usia nasabah dan kecenderungan untuk keluar dari layanan.
- Ini dapat diartikan bahwa semakin tua seseorang, semakin cenderung mereka bertahan dalam layanan.

2. **Korelasi antara Jenis Kelamin (Gender) dan Exited:**
- Korelasi negatif sebesar -0.31 menunjukkan bahwa terdapat hubungan cukup negatif antara jenis kelamin (laki-laki) dan kecenderungan untuk keluar dari layanan.
- Hal ini dapat diartikan bahwa nasabah perempuan mungkin cenderung lebih loyal terhadap layanan dibandingkan dengan nasabah laki-laki.

3. **Korelasi antara Kepemilikan Kartu Kredit (HasCrCard) dan Exited:**
- Korelasi negatif sebesar -0.17 menunjukkan bahwa kepemilikan kartu kredit memiliki pengaruh cukup negatif terhadap kecenderungan keluar dari layanan.
- Artinya, nasabah yang memiliki kartu kredit cenderung lebih setia terhadap layanan.

4. **Korelasi antara Skor Kredit (CreditScore) dan Exited:**
- Korelasi negatif sebesar -0.04 menunjukkan bahwa terdapat hubungan yang kurang kuat antara skor kredit dan kecenderungan keluar dari layanan.
- Hal ini mungkin menandakan bahwa nasabah dengan skor kredit yang lebih tinggi memiliki kecenderungan yang sedikit lebih rendah untuk keluar dari layanan.

5. **Korelasi antara Estimasi Pendapatan (EstimatedSalary) dan Exited:**
- Korelasi positif yang sangat lemah (0.0075) menunjukkan bahwa tidak ada korelasi yang signifikan antara estimasi pendapatan dan kecenderungan keluar dari layanan.
- Dengan kata lain, estimasi pendapatan tidak menjadi faktor utama yang mempengaruhi keputusan nasabah untuk keluar dari layanan.

6. **Korelasi antara Kepemilikan Kartu Kredit (HasCrCard) dan Jenis Kelamin (Gender):**
- Korelasi positif sebesar 0.078 menunjukkan bahwa ada hubungan positif yang kurang kuat antara kepemilikan kartu kredit dan jenis kelamin laki-laki.
- Artinya, laki-laki mungkin sedikit lebih mungkin memiliki kartu kredit.

### 2.2.9 Analisis Distribusi Variabel Numerik

## 2.3 Pra-Pemrosesan Data

### 2.3.1 Mempersiapkan data
"""

#Mempersiapkan data
def data_prepare(df):
    df_prep = df.copy()

    # Encode variabel kategorikal
    df_prep = pd.get_dummies(df_prep, columns=['Gender'], drop_first=True)

    # Handle imbalance data menggunakan SMOTE
    smote = SMOTE(sampling_strategy='auto')
    X_smote, y_smote = smote.fit_resample(df_prep.drop('Exited', axis=1), df_prep['Exited'])

    # Menggabungkan hasil oversampling ke dalam DataFrame
    df_prep = pd.concat([pd.DataFrame(X_smote, columns=df_prep.drop('Exited', axis=1).columns), pd.Series(y_smote, name='Exited')], axis=1)

    return df_prep

# Menyimpan DataFrame yang telah dipersiapkan
df = data_prepare(df)

# Count plot dari kolom target setelah SMOTE
sns.countplot(x='Exited', data=df, palette='Blues')
plt.title('Count Plot dari Kolom Target setelah SMOTE')
plt.show()

# 1
# Menampilkan distribusi variabel dependen terhadap beberapa variabel independen
fig, axarr = plt.subplots(2, 3, figsize=(18, 6))

# Menampilkan distribusi variabel dependen 'Exited' terhadap variabel independen 'Gender_Male'
sns.histplot(x='Gender_Male', hue='Exited', data=df, ax=axarr[0, 0], multiple='stack', kde=True)

# Menampilkan distribusi variabel dependen 'Exited' terhadap variabel independen 'Age'
sns.histplot(x='Age', hue='Exited', data=df, ax=axarr[0, 1], multiple='stack', kde=True)

# Menampilkan distribusi variabel dependen 'Exited' terhadap variabel independen 'CreditScore'
sns.histplot(x='CreditScore', hue='Exited', data=df, ax=axarr[0, 2], multiple='stack', kde=True)

# Menampilkan distribusi variabel dependen 'Exited' terhadap variabel independen 'EstimatedSalary'
sns.histplot(x='EstimatedSalary', hue='Exited', data=df, ax=axarr[1, 0], multiple='stack', kde=True)

# Menampilkan distribusi variabel dependen 'Exited' terhadap variabel independen 'HasCrCard'
sns.histplot(x='HasCrCard', hue='Exited', data=df, ax=axarr[1, 1], multiple='stack', kde=True)

# Menghitung jumlah nilai 0 (tidak Exited) dan nilai 1 (Exited) pada variabel dependen 'Exited'
zero_count, one_count = df['Exited'].value_counts()
print("Distribusi variabel dependen terhadap beberapa variabel independen setelah SMOTE:")
print("Exited 0 count:", zero_count)
print("Exited 1 count:", one_count)

fig.suptitle('Distribusi Variabel Dependen Terhadap Beberapa Variabel Independen Setelah SMOTE', fontsize=18)

# Menampilkan plot
plt.show()

# 2
# Menampilkan distribusi variabel numerik dalam DataFrame
numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
df_num_cols = df.select_dtypes(include=numerics)
columns = df_num_cols.columns[: len(df_num_cols.columns)]

fig = plt.figure()
fig.set_size_inches(18, 15)
length = len(columns)

for i, j in zip(columns, range(length)):
    plt.subplot(int(length / 2), 3, j + 1)
    plt.subplots_adjust(wspace=0.2, hspace=0.5)
    sns.histplot(df_num_cols[i], bins=20, kde=True, color='skyblue', edgecolor='black')
    plt.title(i)

fig.suptitle('Distribusi Variabel Numerik Setelah SMOTE', fontsize=18)
plt.show()

# 3
# Menampilkan distribusi variabel dependen terhadap variabel lainnya
numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
df_dependent_var = df[df['Exited']==1]
df_num_cols = df_dependent_var.select_dtypes(include=numerics)
columns = df_num_cols.columns[:len(df_num_cols.columns)]
fig = plt.figure()
fig.set_size_inches(18, 15)
length = len(columns)
for i, j in zip(columns, range(length)):
    plt.subplot(int(length/2), 3, j+1)
    plt.subplots_adjust(wspace=0.2, hspace=0.5)
    sns.histplot(df_num_cols[i], bins=20, kde=True, color='skyblue', edgecolor='black')
    plt.title(i)
fig.suptitle('Distribusi Variabel Dependen Terhadap Variabel Lainnya Setelah SMOTE', fontsize=18)
plt.show()

"""1. **Gender:**
- Sebelum SMOTE, Female memiliki jumlah yang lebih rendah pada kategori '1' (Exited) dibandingkan dengan Male.
- Setelah SMOTE, perbedaan jumlah antara Female dan Male pada kategori '1' (Exited) menurun, tetapi masih terdapat perbedaan signifikan.

2. **Age:**
- Sebelum SMOTE, kategori umur '19-35' memiliki jumlah Exited yang lebih tinggi dibandingkan dengan '0-18'.
- Setelah SMOTE, terdapat peningkatan jumlah Exited pada kategori '0-18', namun kategori '19-35' masih memiliki jumlah Exited yang tinggi.

3. **CreditScore:**
- Sebelum SMOTE, skor kredit di bawah rata-rata memiliki jumlah Exited yang lebih rendah dibandingkan dengan skor kredit di atas rata-rata.
- Setelah SMOTE, perbedaan jumlah Exited antara skor kredit di bawah rata-rata dan skor kredit di atas rata-rata menurun, tetapi skor kredit di bawah rata-rata masih memiliki jumlah Exited yang lebih rendah.

4. **EstimatedSalary:**
- Sebelum SMOTE, kategori pendapatan di bawah rata-rata dan di atas rata-rata memiliki jumlah Exited yang cukup seimbang.
- Setelah SMOTE, perbedaan jumlah Exited antara kategori pendapatan di bawah rata-rata dan di atas rata-rata tetap seimbang.

5. **HasCrCard:**
- Sebelum SMOTE, terdapat perbedaan yang signifikan antara pemegang kartu kredit (HasCrCard=1) dan bukan pemegang kartu kredit (HasCrCard=0) dalam hal jumlah Exited.
- Setelah SMOTE, perbedaan jumlah Exited antara pemegang kartu kredit dan bukan pemegang kartu kredit menurun, tetapi pemegang kartu kredit (HasCrCard=1) masih memiliki jumlah Exited yang lebih tinggi.

SMOTE berhasil menyeimbangkan jumlah sampel antara kelas Exited (1) dan kelas Not Exited (0), mengurangi ketidakseimbangan kelas yang dapat memengaruhi kinerja model klasifikasi.

### 2.3.2 Handling Missing Values
"""

def process_and_display_data(df):
    # Menangani nilai yang hilang
    missing_value_len = df.isnull().any().sum()
    if missing_value_len == 0:
        print("Tidak ada nilai yang hilang.")
    else:
        print(f"Investigasi nilai yang hilang. Jumlah nilai yang hilang: {missing_value_len}")
    print("\n")

    print("Jumlah unik untuk setiap variabel")
    # Menampilkan jumlah unik untuk setiap variabel yang telah dipersiapkan
    for index, value in df.nunique().items():
        print(f"{index}\n\t\t\t:{value}")
    return df

result_df = process_and_display_data(df)

"""### 2.3.3 Handling Outliers"""

def handle_outliers(df):
    # Deteksi outlier dengan metode LOF
    lof = LocalOutlierFactor()
    outliers = lof.fit_predict(df.drop('Exited', axis=1))
    # Hapus baris yang teridentifikasi outlier
    df_no_outliers = df.loc[outliers != -1]

    return df_no_outliers

df = handle_outliers(df)

# Menampilkan informasi setelah handling outliers
result_df = process_and_display_data(df)

"""1. **Jumlah Baris:**
- Sebelum handling outlier, jumlah baris (CustomerId) adalah 15546, sedangkan setelah menggunakan LOF, jumlah baris berkurang menjadi 15532.
- Hal ini menunjukkan bahwa ada beberapa baris yang diidentifikasi sebagai outlier dan dihapus dari dataset.

2. **EstimatedSalary:**
- Sebelum handling outlier, jumlah nilai unik pada variabel EstimatedSalary adalah 15880, dan setelah menggunakan LOF, nilai ini berkurang menjadi 15865.
- Hal ini menunjukkan bahwa ada beberapa nilai yang dianggap sebagai outlier pada variabel EstimatedSalary.

3. **Konsistensi pada Variabel Lain:**
- Variabel lain seperti Age, CreditScore, HasCrCard, Gender_Male, dan Exited tidak mengalami perubahan signifikan dalam jumlah nilai unik setelah handling outlier.
- Hal ini menunjukkan bahwa LOF lebih fokus pada identifikasi outlier pada variabel tertentu (dalam hal ini, EstimatedSalary).
"""

# Sebelum Penanganan Outlier
numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
df_num_cols_before = df.select_dtypes(include=numerics)
columns_before = df_num_cols_before.columns[: len(df_num_cols_before.columns)]

fig_before = plt.figure()
fig_before.set_size_inches(18, 15)
length_before = len(columns_before)

for i, j in zip(columns_before, range(length_before)):
    plt.subplot(int(length_before / 2), 3, j + 1)
    plt.subplots_adjust(wspace=0.2, hspace=0.5)
    df_num_cols_before[i].hist(bins=20, edgecolor='black')
    plt.title(f'Sebelum Penanganan Outlier - {i}')

fig_before.suptitle('Perbandingan Distribusi Fitur Numerik Sebelum dan Sesudah Penanganan Outlier', fontsize=18)
plt.show()

# Setelah Penanganan Outlier
df_no_outliers = handle_outliers(df)

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
df_num_cols_after = df_no_outliers.select_dtypes(include=numerics)
columns_after = df_num_cols_after.columns[: len(df_num_cols_after.columns)]

fig_after = plt.figure()
fig_after.set_size_inches(18, 15)
length_after = len(columns_after)

for i, j in zip(columns_after, range(length_after)):
    plt.subplot(int(length_after / 2), 3, j + 1)
    plt.subplots_adjust(wspace=0.2, hspace=0.5)
    df_num_cols_after[i].hist(bins=20, edgecolor='black')
    plt.title(f'Setelah Penanganan Outlier - {i}')

fig_after.suptitle('Perbandingan Distribusi Fitur Numerik Sebelum dan Sesudah Penanganan Outlier', fontsize=18)
plt.show()

# Sebelum Penanganan Outlier
fig_before_box = plt.figure()
fig_before_box.set_size_inches(18, 8)
sns.set(style="whitegrid")

for i, column in enumerate(columns_before):
    plt.subplot(2, 3, i + 1)
    sns.boxplot(x=df_num_cols_before[column], color='skyblue')
    plt.title(f'Boxplot Sebelum Penanganan Outlier - {column}')

fig_before_box.suptitle('Perbandingan Boxplot Fitur Numerik Sebelum Penanganan Outlier', fontsize=18)
plt.show()

# Setelah Penanganan Outlier
fig_after_box = plt.figure()
fig_after_box.set_size_inches(18, 8)

for i, column in enumerate(columns_after):
    plt.subplot(2, 3, i + 1)
    sns.boxplot(x=df_num_cols_after[column], color='lightcoral')
    plt.title(f'Boxplot Setelah Penanganan Outlier - {column}')

fig_after_box.suptitle('Perbandingan Boxplot Fitur Numerik Setelah Penanganan Outlier', fontsize=18)
plt.show()

"""# 3. Model Training

PIC : Timmy, Eko

## 3.1 Splitting & Scaling Data
"""

def model_prepare(df_model):
    y = df_model[dependent_variable_name]
    X = df_model.loc[:, df_model.columns != dependent_variable_name]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
    sc = StandardScaler()
    X_train = sc.fit_transform(X_train)
    X_test = sc.transform (X_test)
    return X_train, X_test, y_train, y_test

"""## 3.2 Model Training"""

def data_training(X_train, X_test, y_train, y_test):

    models = []
    models.append(('LOGR', LogisticRegression()))
    models.append(('KNN', KNeighborsClassifier()))
    models.append(('CART', DecisionTreeClassifier()))
    models.append(('RF', RandomForestClassifier()))
    models.append(('GBM', GradientBoostingClassifier()))
    models.append(('XGBoost', XGBClassifier()))
    models.append(('LightGBM', LGBMClassifier()))
    models.append(('SVM', SVC()))
    models.append(('AdaBoost', AdaBoostClassifier()))

    res_cols = ["model", "accuracy_score", "0_precision", "0_recall", "1_precision", "1_recall"]
    df_result = pd.DataFrame(columns=res_cols)
    index = 0
    for name, model in models:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        score = accuracy_score(y_test, y_pred)
        class_report = classification_report(y_test, y_pred, digits=2, output_dict=True)
        zero_report = class_report['0']
        one_report = class_report['1']

        idx_res_values = [name, score, zero_report['precision'], zero_report['recall'], one_report['precision'], one_report['recall']]
        # df_result.at[index, res_cols] = idx_res_values
        df_result.loc[index, res_cols] = idx_res_values
        index += 1
    return df_result.sort_values('accuracy_score', ascending=False)
    # df_result = df_result.sort_values("accuracy_score", ascending=False).reset_index(drop=True)
    # return df_result

# Model_prepare test, train split 0.2
X_train, X_test, y_train, y_test = model_prepare(df_model=df)

training_result = data_training(X_train, X_test, y_train, y_test)
training_result

"""## 3.2.1 | Logistic Regression"""

# Create the Logistic Regression model
log_reg_model = LogisticRegression(C=1, penalty='l2', solver='liblinear', max_iter=200)

# Fit the SVM model to the training data
log_reg_model.fit(X_train, y_train)

# Make predictions on the training data
y_train_pred = log_reg_model.predict(X_train)

# Make predictions on the validation data
y_test_pred = log_reg_model.predict(X_test)

# Calculate the training accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)

# Calculate the validation accuracy
val_accuracy = accuracy_score(y_test, y_test_pred)

print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

# Create confusion matrices
train_confusion = confusion_matrix(y_train, y_train_pred, normalize='true')
val_confusion = confusion_matrix(y_test, y_test_pred, normalize='true')

# Plot the confusion matrices
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.heatmap(train_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Training)')

plt.subplot(1, 2, 2)
sns.heatmap(val_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Validation)')
plt.show()

"""## 3.2.2 | KNN"""

# Create a KNN classifier (you can adjust the number of neighbors 'n_neighbors')
knn_model = KNeighborsClassifier(n_neighbors=5)

# Fit the SVM model to the training data
knn_model.fit(X_train, y_train)

# Make predictions on the training data
y_train_pred = knn_model.predict(X_train)

# Make predictions on the validation data
y_test_pred = knn_model.predict(X_test)

# Calculate the training accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)

# Calculate the validation accuracy
val_accuracy = accuracy_score(y_test, y_test_pred)

print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

# Create confusion matrices
train_confusion = confusion_matrix(y_train, y_train_pred, normalize='true')
val_confusion = confusion_matrix(y_test, y_test_pred, normalize='true')

# Plot the confusion matrices
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.heatmap(train_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Training)')

plt.subplot(1, 2, 2)
sns.heatmap(val_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Validation)')
plt.show()

"""<a id="3.3.2"></a>
> <span style='font-size:15px; font-family:Verdana;color: #254E58;'><b> Hyperparameter Tuning of KNN </b></span>
"""

# Define the hyperparameter grid to search
param_grid = {
    'n_neighbors': [1, 3, 5, 7, 9]  # Adjust the number of neighbors to explore
}

# Create the KNN classifier
knn_model = KNeighborsClassifier()

# Perform GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Get the best estimator with tuned hyperparameters
best_model = grid_search.best_estimator_

# Make predictions on the training data using the best model
y_train_pred = best_model.predict(X_train)

# Make predictions on the validation data using the best model
y_test_pred = best_model.predict(X_test)

# Calculate the training and validation accuracies
train_accuracy = accuracy_score(y_train, y_train_pred)
val_accuracy = accuracy_score(y_test, y_test_pred)

print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

# Create confusion matrices
train_confusion = confusion_matrix(y_train, y_train_pred, normalize='true')
val_confusion = confusion_matrix(y_test, y_test_pred, normalize='true')

# Plot the confusion matrices
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.heatmap(train_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Training)')

plt.subplot(1, 2, 2)
sns.heatmap(val_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Validation)')
plt.show()

"""## 3.2.3 | Decision Tree"""

# Create the Decision Tree model
decision_tree_model = DecisionTreeClassifier(random_state=42)

# Fit the SVM model to the training data
decision_tree_model.fit(X_train, y_train)

# Make predictions on the training data
y_train_pred = decision_tree_model.predict(X_train)

# Make predictions on the validation data
y_test_pred = decision_tree_model.predict(X_test)

# Calculate the training accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)

# Calculate the validation accuracy
val_accuracy = accuracy_score(y_test, y_test_pred)

print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

# Create confusion matrices
train_confusion = confusion_matrix(y_train, y_train_pred, normalize='true')
val_confusion = confusion_matrix(y_test, y_test_pred, normalize='true')

# Plot the confusion matrices
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.heatmap(train_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Training)')

plt.subplot(1, 2, 2)
sns.heatmap(val_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Validation)')
plt.show()

"""<a id="3.3.3"></a>
> <span style='font-size:15px; font-family:Verdana;color: #254E58;'><b> Hyperparameter Tuning Of Decision Tree </b></span>
"""

param_grid = {
    'max_depth': [None, 5, 10, 15, 20],
    'min_samples_split': [2, 5, 10, 15, 20, 25],
    'min_samples_leaf': [1, 3, 5, 7],
    'criterion': ['gini', 'entropy']  # Add criterion hyperparameter
}

# Create the Decision Tree model
decision_tree_model = DecisionTreeClassifier(random_state=42)

# Perform GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(decision_tree_model, param_grid, cv=5, n_jobs=-1, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Get the best estimator with tuned hyperparameters
best_model = grid_search.best_estimator_

# Fit the final model to the training data
best_model.fit(X_train, y_train)

# Evaluate the final model on the training and validation data
train_accuracy = best_model.score(X_train, y_train)
val_accuracy = best_model.score(X_test, y_test)

# Print the results
print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

# Create confusion matrices
train_confusion = confusion_matrix(y_train, y_train_pred, normalize='true')
val_confusion = confusion_matrix(y_test, y_test_pred, normalize='true')

# Plot the confusion matrices
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.heatmap(train_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Training)')

plt.subplot(1, 2, 2)
sns.heatmap(val_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Validation)')
plt.show()

"""## 3.2.4 | Random Forest"""

# Create the Random Forest model
random_forest_model = RandomForestClassifier(n_jobs =-1, random_state = 42)

# Fit the SVM model to the training data
random_forest_model.fit(X_train, y_train)

# Make predictions on the training data
y_train_pred = random_forest_model.predict(X_train)

# Make predictions on the validation data
y_test_pred = random_forest_model.predict(X_test)

# Calculate the training accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)

# Calculate the validation accuracy
val_accuracy = accuracy_score(y_test, y_test_pred)

print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

# Create confusion matrices
train_confusion = confusion_matrix(y_train, y_train_pred, normalize='true')
val_confusion = confusion_matrix(y_test, y_test_pred, normalize='true')

# Plot the confusion matrices
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.heatmap(train_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Training)')

plt.subplot(1, 2, 2)
sns.heatmap(val_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Validation)')
plt.show()

"""<a id="3.3.4"></a>
> <span style='font-size:15px; font-family:Verdana;color: #254E58;'><b>Hyperparameter Tuning of Random Forest </b></span>
"""

param_grid = {
    'n_estimators': [10, 20, 30],  # Adjust the number of trees in the forest
    'max_depth': [10, 20, 30],  # Adjust the maximum depth of each tree
    'min_samples_split': [2, 5, 10, 15, 20],  # Adjust the minimum samples required to split a node
    'min_samples_leaf': [1, 2, 4, 6, 8]  # Adjust the minimum samples required in a leaf node
}

# Create the Random Forest model
random_forest_model = RandomForestClassifier(random_state=42, n_jobs=-1)

# Perform GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(random_forest_model, param_grid, cv=5, n_jobs=-1, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Get the best estimator with tuned hyperparameters
best_model = grid_search.best_estimator_

# Fit the final model to the training data
best_model.fit(X_train, y_train)

# Evaluate the model on the training and validation data
train_accuracy = best_model.score(X_train, y_train)
val_accuracy = best_model.score(X_test, y_test)

# Print the results
print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

# Plot the confusion matrices
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.heatmap(train_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Training)')

plt.subplot(1, 2, 2)
sns.heatmap(val_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Validation)')
plt.show()

"""## 3.2.5 | Support Vector Classifier"""

# Create the SVM model
svm_model = SVC(kernel='linear')

# Fit the SVM model to the training data
svm_model.fit(X_train, y_train)

# Make predictions on the training data
y_train_pred = svm_model.predict(X_train)

# Make predictions on the validation data
y_val_pred = svm_model.predict(X_test)

# Calculate the training accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)

# Calculate the validation accuracy
val_accuracy = accuracy_score(y_test, y_test_pred)

print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

# Create confusion matrices
train_confusion = confusion_matrix(y_train, y_train_pred, normalize='true')
val_confusion = confusion_matrix(y_test, y_test_pred, normalize='true')

# Plot the confusion matrices
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.heatmap(train_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Training)')

plt.subplot(1, 2, 2)
sns.heatmap(val_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Validation)')
plt.show()

"""## 3.2.6 | AdaBoost Classifier"""

# Create an AdaBoost classifier model
adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42)

# Fit the SVM model to the training data
adaboost_model.fit(X_train, y_train)

# Make predictions on the training data
y_train_pred = adaboost_model.predict(X_train)

# Make predictions on the validation data
y_val_pred = adaboost_model.predict(X_test)

# Calculate the training accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)

# Calculate the validation accuracy
val_accuracy = accuracy_score(y_test, y_test_pred)

print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

# Create confusion matrices
train_confusion = confusion_matrix(y_train, y_train_pred, normalize='true')
val_confusion = confusion_matrix(y_test, y_test_pred, normalize='true')

# Plot the confusion matrices
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.heatmap(train_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Training)')

plt.subplot(1, 2, 2)
sns.heatmap(val_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Validation)')
plt.show()

# Create an AdaBoost classifier model
adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42)

# Fit the AdaBoost model to the training data
adaboost_model.fit(X_train, y_train)

# Make predictions on the training data
y_train_pred_adaboost = adaboost_model.predict(X_train)

# Make predictions on the validation data
y_test_pred_adaboost = adaboost_model.predict(X_test)

# Calculate the training accuracy
train_accuracy_adaboost = accuracy_score(y_train, y_train_pred_adaboost)

# Calculate the validation accuracy
val_accuracy_adaboost = accuracy_score(y_test, y_test_pred_adaboost)

# Print the training and validation accuracies
print("AdaBoost Training Accuracy:", train_accuracy_adaboost)
print("AdaBoost Validation Accuracy:", val_accuracy_adaboost)

# Create a confusion matrix for validation
confusion_adaboost = confusion_matrix(y_test, y_test_pred_adaboost)

# Plot the confusion matrix
plt.figure()
sns.heatmap(confusion_adaboost, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('AdaBoost Confusion Matrix (Validation)')
plt.show()

"""## 3.2.7 | Gradient Boosting Classifier"""

# Create a Gradient Boosting classifier
gbm_model = GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)

# Fit the SVM model to the training data
gbm_model.fit(X_train, y_train)

# Make predictions on the training data
y_train_pred = gbm_model.predict(X_train)

# Make predictions on the validation data
y_val_pred = gbm_model.predict(X_test)

# Calculate the training accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)

# Calculate the validation accuracy
val_accuracy = accuracy_score(y_test, y_test_pred)

print("Training Accuracy:", train_accuracy)
print("Validation Accuracy:", val_accuracy)

# Create confusion matrices
train_confusion = confusion_matrix(y_train, y_train_pred, normalize='true')
val_confusion = confusion_matrix(y_test, y_test_pred, normalize='true')

# Plot the confusion matrices
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.heatmap(train_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Training)')

plt.subplot(1, 2, 2)
sns.heatmap(val_confusion, annot=True, fmt='.2f', cmap='Blues', cbar=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Validation)')
plt.show()

# Create a Gradient Boosting classifier
gbm_model = GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)

# Fit the GBM model to the training data
gbm_model.fit(X_train, y_train)

# Make predictions on the training data
y_train_pred_gbm = gbm_model.predict(X_train)

# Make predictions on the validation data
y_test_pred_gbm = gbm_model.predict(X_test)

# Calculate the training accuracy
train_accuracy_gbm = accuracy_score(y_train, y_train_pred_gbm)

# Calculate the validation accuracy
val_accuracy_gbm = accuracy_score(y_test, y_test_pred_gbm)

# Print the training and validation accuracies
print("GBM Training Accuracy:", train_accuracy_gbm)
print("GBM Validation Accuracy:", val_accuracy_gbm)

"""## 3.2.8 | XGBoost Classifier"""

# Create an XGBoost classifier
xgboost_model = XGBClassifier(n_estimators=100, max_depth=3, random_state=42)

# Fit the XGBoost model to the training data
xgboost_model.fit(X_train, y_train)

# Make predictions on the training data
y_train_pred_xgboost = xgboost_model.predict(X_train)

# Make predictions on the validation data
y_test_pred_xgboost = xgboost_model.predict(X_test)

# Calculate the training accuracy
train_accuracy_xgboost = accuracy_score(y_train, y_train_pred_xgboost)

# Calculate the validation accuracy
val_accuracy_xgboost = accuracy_score(y_test, y_test_pred_xgboost)

# Print the training and validation accuracies
print("XGBoost Training Accuracy:", train_accuracy_xgboost)
print("XGBoost Validation Accuracy:", val_accuracy_xgboost)

"""<a id="3.3.8"></a>
> <span style='font-size:15px; font-family:Verdana;color: #254E58;'><b>Hyperparameter Tuning of XGBoost</b></span>
"""

xgb_model=XGBClassifier(
    learning_rate=0.23, max_delta_step=5,
    objective='reg:logistic', n_estimators=92,
    max_depth=5, eval_metric="logloss", gamma=3, base_score=0.5)

xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_test)
print(classification_report(y_test, y_pred, digits=2))
print("Accuracy score of Tuned XGBoost Regression: ", accuracy_score(y_test, y_pred))

"""## 3.2.9 | LightGBM Classifier"""

# Create a Gradient Boosting classifier
lgbm_model = LGBMClassifier(n_estimators=100, max_depth=3, random_state=42)

# Fit the GBM model to the training data
lgbm_model.fit(X_train, y_train)

# Make predictions on the training data
y_train_pred_lgbm = lgbm_model.predict(X_train)

# Make predictions on the validation data
y_test_pred_lgbm = lgbm_model.predict(X_test)

# Calculate the training accuracy
train_accuracy_lgbm = accuracy_score(y_train, y_train_pred_lgbm)

# Calculate the validation accuracy
val_accuracy_lgbm = accuracy_score(y_test, y_test_pred_lgbm)

# Print the training and validation accuracies
print("LightGBM Training Accuracy:", train_accuracy_lgbm)
print("LightGBM Validation Accuracy:", val_accuracy_lgbm)

"""<a id="3.3.9"></a>
> <span style='font-size:15px; font-family:Verdana;color: #254E58;'><b>Hyperparameter Tuning of LightGBM</b></span>
"""

lgbm_model = LGBMClassifier(
    silent = 0, learning_rate = 0.09, max_delta_step = 2,
    n_estimators = 100, boosting_type = 'gbdt',
    max_depth = 10, eval_metric = "logloss",
    gamma = 3, base_score = 0.5
)

lgbm_model.fit(X_train, y_train)
y_pred = lgbm_model.predict(X_test)
print(classification_report(y_test, y_pred, digits=2))
print("Accuracy score of tuned LightGBM model: ", accuracy_score(y_test, y_pred))

"""# 4. Model Deployment

PIC : Kemas

## 4.1 Streamlit App
"""

import streamlit as st
import streamlit.components.v1 as stc
import pickle
import pandas as pd
import numpy as np

with open('model/Random_Forest_model.pkl','rb') as file:
    Random_Forest_Model = pickle.load(file)

def main():
    # stc.html(html_temp)
    st.title("Customer Churn Prediction App")
    st.caption("This app is created by Algowizard Team for Final Project of Data Science Bootcamp")

    menu = ["Home","Machine Learning"]
    choice = st.sidebar.selectbox("Menu", menu)

    if choice == "Home":
        st.header("Home")
        st.caption("Aplikasi prediksi churn memanfaatkan pembelajaran mesin dan kecerdasan buatan untuk menganalisis data pelanggan dan mengidentifikasi mereka yang berisiko pergi. Hal ini memungkinkan bisnis untuk secara proaktif melibatkan pelanggan ini dengan intervensi yang ditargetkan dan strategi retensi, meminimalkan churn dan meningkatkan nilai umur pelanggan.")

        st.markdown("""
            <p style="font-size: 16px; font-weight: bold">Sekilas tentang Dataset yang digunakan</p>
            """, unsafe_allow_html=True)

        df = pd.DataFrame(np.random.randn(10, 5), columns=("col %d" % i for i in range(5)))
        st.table(df)


    elif choice == "Machine Learning":
        st.header("Prediction Model")
        run_ml_app()

    col1, col2, col3 = st.columns([1, 10, 1])  # Center column takes up most of the width
    with col2:
        images = ["1. Ola.png", "2. July.png", "3. Faza.png","4. Timmy.png",
              "5. Kemas.png", "6. Eko.png", "7. Osha.png"]
        st.image(images, width=80)  # Set width for each image

def run_ml_app():
    # design = """<div style='padding:15px;">
    #                 <h1 style='color:#fff'>Loan Eligibility Prediction</h1>
    #             </div>"""
    # st.markdown(design, unsafe_allow_html=True)

    st.markdown("""
    <p style="font-size: 16px; font-weight: bold">Insert Data</p>
    """, unsafe_allow_html=True)

    left, right = st.columns((2,2))
    gender = left.selectbox('Gender',
                            ('Male', 'Female'))
    age = left.number_input('Age', 1, 100)
    credit_score = left.number_input('Credit Score',0,1000)
    estimated_salary = right.number_input('Estimated Salary',0.0,100000000.00)
    has_credit_card = right.selectbox('Credit Card',('Yes','No'))

    # married = right.selectbox('Married', ('Yes','No'))
    # dependent = left.selectbox('Dependents', ('None', 'One', 'Two', 'Three'))
    # education = right.selectbox('Education', ('Graduate', 'Non-Graduate'))
    # self_employed = left.selectbox('Self-Employed', ('Yes', 'No'))
    # applicant_income = right.number_input('Applicant Income')
    # coApplicant_income = left.number_input(
    #     'Co - Applicant Income')
    # loan_amount = right.number_input('Loan Amount')
    # loan_amount_term = left.number_input('Loan Tenor (In Months)')
    # credit_history = right.number_input('Credit History', 0.0, 1.0)
    # property_area = st.selectbox('Property Area', ('Semiurban','Urban', 'Rural'))
    button = st.button('Predict')

    #if button is clicked (ketika button dipencet)
    if button:
        #make prediction
        result = predict(gender,age,credit_score,estimated_salary,has_credit_card)
        if result == 'Eligible':
            st.success(f'You are {result} for the loan')
        else:
            st.warning(f'You are {result} for the loan')


def predict(gender,age,credit_score,estimated_salary,has_credit_card):
    #processing user input
    gen = 0 if gender == 'Male' else 1
    cre = 0 if has_credit_card == 'No' else 1
    # mar = 0 if married == 'Yes' else 1
    # dep = float(0 if dependent == 'None' else 1 if dependent == 'One' else 2 if dependent == 'Two' else 3)
    # edu = 0 if education == 'Graduate' else 1
    # sem = 0 if self_employed == 'Yes' else 1
    # pro = 0 if property_area == 'Semiurban' else 1 if property_area == 'Urban' else 2
    # lam = loan_amount/1000
    # cap = coApplicant_income / 1000

    #Making prediction
    prediction = Random_Forest_Model.predict([[gen, cre, age, credit_score,
                                               estimated_salary]])
    result = 'Stayed' if prediction == 0 else 'Exited'

    return result

if __name__ == "__main__":
    main()